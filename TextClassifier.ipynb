{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f2fbac-081e-4ab5-823f-f76b3c05415f",
   "metadata": {},
   "source": [
    "# Text As Data Coursework\n",
    "<font color='blue'>**Total Marks - 58**</font>"
   ]
  },
  {
   "attachments": {
    "f62d7bf3-e6e2-4d12-b6c1-6f06984a4111.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ea8e3d49-64ed-4c2a-abf0-37581e3520f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Introduction** <br>\n",
    "\n",
    "The TaD coding coursework aims to assess your abilities to perform text processing techniques as applied to a \n",
    "multi-class classification problem. <br>\n",
    "Your work will be submitted through a Moodle quiz. For each question, you should submit your text answer \n",
    "(providing the required information) separately to your code. \n",
    "\n",
    "**Task:** <br>\n",
    "A museum records organisation has a large set of records that need to be assigned to one of five \n",
    "institutions (in the table below). They have provided a small set of data to be used as the training, validation and test \n",
    "set. Our goal is to build a classifier that could assign an unseen record to the correct institution. \n",
    "\n",
    "![Screenshot 2025-03-06 154808.png](attachment:f62d7bf3-e6e2-4d12-b6c1-6f06984a4111.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc77446-c86a-40c9-863d-14533f68d384",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b3294c-6b9a-4071-ae39-5a95db119ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c80b7ee-fec2-4243-afd7-2e85b778a420",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "*<font color='Purple'>Marks - 9</font>*\n",
    "\n",
    "**Task** -\n",
    "- Download and load the dataset. There are some issues with the training split of the data that would stop it being used to train a classifier.\n",
    "- Report all issues and how you fixed them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97cfcea9-b1fe-4a02-8371-e93a625c71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "with open('./Data/dataset.json', 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae642a4-c4f9-4e2e-b9b6-41a6bb971523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LLM Prompt Template 1\n",
    "with open('./Data/llm_prompt_template_1.json', 'r') as file:\n",
    "    LLM_PromptTemplate1 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c4f687-a16e-4376-8c1b-36fcd9cf93d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LLM Prompt Template 2\n",
    "with open('./Data/llm_prompt_template_2.json', 'r') as file:\n",
    "    LLM_PromptTemplate2 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5bbd45-f944-4759-b2cd-180518567bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LLM Prompt Template 3\n",
    "with open('./Data/llm_prompt_template_3.json', 'r') as file:\n",
    "    LLM_PromptTemplate3 = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59636a0b-6468-4eb1-a8c4-f215f63d559a",
   "metadata": {},
   "source": [
    "Just looking at the dataset, I can tell that we have two apparent problems.\n",
    "\n",
    "First, different-different keys for similar fields. For example:\n",
    "- We have json objects, with similar fields such as *key*, *label*, and *labl*\n",
    "- Similarly, we have json objects, with similar fields such as *content*, *text*, and *description*\n",
    "\n",
    "Second problem:\n",
    "- Labels are wrong, sometimes it has integer value and sometimes it has a text value.\n",
    "\n",
    "Third problem:\n",
    "- A few samples of the training datasets are None type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d33f5f22-a398-4413-983c-6f480c169e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSet = data[\"train\"]\n",
    "validationSet = data[\"val\"]\n",
    "testSet = data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd8d2c3-9f17-46b6-a316-80795fb5b928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique keys in Training dataset:  {'label', 'key', 'id', 'text', 'description', 'content', 'labl'}\n"
     ]
    }
   ],
   "source": [
    "uniqueKeys = set()\n",
    "for sample in trainingSet:\n",
    "    for x in sample.keys():\n",
    "        uniqueKeys.add(x)\n",
    "print(\"Unique keys in Training dataset: \", uniqueKeys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d29ad-f818-416d-a232-ecaf876a2f9b",
   "metadata": {},
   "source": [
    "Unique Keys: {'label', 'id', 'text', 'labl', 'content', 'key', 'description'} <br>\n",
    "This confirms our suspicion for the First problem.\n",
    "\n",
    "**Solution**\n",
    "- We should choose a single key for each such fields and stick with it.\n",
    "  - Choosing \"*content*\" and \"*label*\"\n",
    "    - **Reason**: Both Validation and Test set has these two keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e67d2c5b-e17e-49c8-bfb5-6759a4f93bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(trainingSet)):\n",
    "    dictKeys = trainingSet[i].keys()\n",
    "    if(\"labl\" in dictKeys):\n",
    "        trainingSet[i][\"label\"] = trainingSet[i].pop(\"labl\")\n",
    "    elif(\"key\" in dictKeys):\n",
    "        trainingSet[i][\"label\"] = trainingSet[i].pop(\"key\")\n",
    "\n",
    "    if(\"description\" in dictKeys):\n",
    "        trainingSet[i][\"content\"] = trainingSet[i].pop(\"description\")\n",
    "    elif(\"text\" in dictKeys):\n",
    "        trainingSet[i][\"content\"] = trainingSet[i].pop(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329308db-1fa6-472d-8e76-2f4ce7a4c58c",
   "metadata": {},
   "source": [
    "Now, let's again check unique number of keys we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48448308-7b8a-4db3-8fd7-27350b23b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique keys in Training dataset:  {'label', 'content', 'id'}\n"
     ]
    }
   ],
   "source": [
    "uniqueKeys = set()\n",
    "for sample in trainingSet:\n",
    "    for x in sample.keys():\n",
    "        uniqueKeys.add(x)\n",
    "print(\"Unique keys in Training dataset: \", uniqueKeys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf2252-9731-4830-a5fb-78aef683709b",
   "metadata": {},
   "source": [
    "Now, that we have solved the first problem, let's focus on the second problem. The problem is that, for label field we have both integer and text values. Sometimes, even those text values doesn't match with the Institution names provided. <br>\n",
    "**Solution**:\n",
    "- First, convert wrong text values to correct values.\n",
    "- Then, convert all text values to an integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63cb2b96-0e93-4bbf-863e-771dfc95a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "actualClasses = [\n",
    "    \"national maritime museum\",\n",
    "    \"national railway museum\",\n",
    "    \"royal botanic gardens, kew\",\n",
    "    \"royal college of physicians of london\",\n",
    "    \"shakespeare birthplace trust\"\n",
    "]\n",
    "classDict = {\n",
    "    \"national maritime museum\": 0,\n",
    "    \"national railway museum\": 1,\n",
    "    \"royal botanic gardens, kew\": 2,\n",
    "    \"royal college of physicians of london\": 3,\n",
    "    \"shakespeare birthplace trust\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bbd68d0-58c6-4ddc-a9b8-fc70ef870f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(trainingSet)):\n",
    "    if(str(trainingSet[i][\"label\"]).lower() not in actualClasses and type(trainingSet[i][\"label\"]) == str):\n",
    "        if(\"maritime\" in trainingSet[i][\"label\"].lower()):\n",
    "            trainingSet[i][\"label\"] = actualClasses[0]\n",
    "        elif(\"railway\" in data[\"train\"][i][\"label\"].lower()):\n",
    "            trainingSet[i][\"label\"] = actualClasses[1]\n",
    "        elif(\"botanic\" in trainingSet[i][\"label\"].lower() or \"kew\" in trainingSet[i][\"label\"].lower()):\n",
    "            trainingSet[i][\"label\"] = actualClasses[2]\n",
    "        elif(\"london\" in trainingSet[i][\"label\"].lower() or \"physicians\" in trainingSet[i][\"label\"].lower()):\n",
    "            trainingSet[i][\"label\"] = actualClasses[3]\n",
    "        elif(\"shakespeare\" in trainingSet[i][\"label\"].lower() or \"birthplace\" in trainingSet[i][\"label\"].lower()):\n",
    "            trainingSet[i][\"label\"] = actualClasses[4]\n",
    "        else:\n",
    "            print(\"-----------------------\")\n",
    "            print(\"HERE!! NO CLASS FOUND!!\")\n",
    "            print(\"Label: \", trainingSet[i][\"label\"])\n",
    "\n",
    "    if(type(trainingSet[i][\"label\"]) == str and str(trainingSet[i][\"label\"]).lower() in actualClasses):\n",
    "        trainingSet[i][\"label\"] = classDict[trainingSet[i][\"label\"].lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f811f73c-3114-4cba-a6fe-0dd43c43b2cf",
   "metadata": {},
   "source": [
    "Now, let's check if all the labels exist between 0 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49a09d00-761b-44e2-80c2-b869543a953e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels in Training Set:  {0, 1, 2, 3, 4}\n"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "for i in range(0, len(trainingSet)):\n",
    "    labels.add(trainingSet[i][\"label\"])\n",
    "print(\"Unique Labels in Training Set: \", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153f4b9-690c-436e-ba76-90230764c5b8",
   "metadata": {},
   "source": [
    "To solve the third problem, we have to remove those samples. <br>\n",
    "**Solution** - \n",
    "- Convert all content to string values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32a060f3-0c9b-443d-8917-4daf5f43cc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '7ab43067-856c-5cee-8574-ac59a24de44b', 'label': 2, 'content': None}\n",
      "{'id': '7148fce2-2315-1826-2615-05abd9b0b806', 'label': 1, 'content': 258157}\n",
      "{'id': 'aa985a46-605f-2be7-12ba-9204d8d240e4', 'content': 829308, 'label': 4}\n",
      "{'id': '95a0d569-602a-5bdc-06ca-62eb08302d3d', 'label': 1, 'content': None}\n",
      "{'id': '246aeb01-6686-57ab-9f4e-3a15f01446e5', 'label': 3, 'content': 535970}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while(i < len(trainingSet)):\n",
    "    if(type(trainingSet[i]['content']) != str):\n",
    "        print(trainingSet[i])\n",
    "        trainingSet.remove(trainingSet[i])\n",
    "    else:\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a56cc23-2b62-4477-b7ed-d3f328eccd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training Set:  150\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Training Set: \", len(trainingSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2e490-89bf-4411-bc63-c246fdf00c0e",
   "metadata": {},
   "source": [
    "Let's fix the problem in Validation and Test Set, since both have labels in STRING format. <br>\n",
    "**Solution** -\n",
    "- Convert them to Integer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "861fa0b0-70a4-4f62-830f-35e08f59e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(validationSet)):\n",
    "    if(validationSet[i]['label'].lower() in classDict.keys()):\n",
    "        validationSet[i]['label'] = classDict[validationSet[i]['label'].lower()]\n",
    "    else:\n",
    "        print(\"This should never run. ALERT!! ALERT!!\")\n",
    "        print(validationSet[i]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ffee8b4-1185-4a91-a66a-9588a8ed1df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(testSet)):\n",
    "    if(testSet[i]['label'].lower() in classDict.keys()):\n",
    "        testSet[i]['label'] = classDict[testSet[i]['label'].lower()]\n",
    "    else:\n",
    "        print(\"This should never run. ALERT!! ALERT!!\")\n",
    "        print(testSet[i]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544651bc-d209-4832-88d6-fedbf5b3aae6",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "*<font color='Purple'>Marks - 5</font>*\n",
    "\n",
    "**Task** - <br>Once the training set has been fixed, report the following:\n",
    "- The sample counts for the training, validation and test sets\n",
    "- The percentage splits for training, validation and test sets\n",
    "- The minimum and maximum length (in characters) of the texts.\n",
    "  - Report separately for the training, validation and test sets\n",
    "- The most frequent five tokens in each class (after tokenizing with `text_pipeline_spacy` from Lab 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944a44e-888f-4d5e-8055-28af380a610d",
   "metadata": {},
   "source": [
    "### Sample Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2b7817c-3562-4df5-a323-72995b847074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Count in Training Set:  150\n",
      "Sample Count in Validation Set:  50\n",
      "Sample Count in Test Set:  50\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Count in Training Set: \", len(trainingSet))\n",
    "print(\"Sample Count in Validation Set: \", len(validationSet))\n",
    "print(\"Sample Count in Test Set: \", len(testSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759975ae-d1f4-45a1-b724-4e770b31e0cf",
   "metadata": {},
   "source": [
    "### Percentage Split between Training, Validation and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db819113-c124-42e6-829c-695d89d62dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Training Set from all dataset:  60.0\n",
      "Percentage of Validation Set from all dataset:  20.0\n",
      "Percentage of Test Set from all dataset:  20.0\n"
     ]
    }
   ],
   "source": [
    "totalSamples = len(trainingSet) + len(validationSet) + len(testSet)\n",
    "\n",
    "print(\"Percentage of Training Set from all dataset: \", round(len(trainingSet)/totalSamples * 100, 2))\n",
    "print(\"Percentage of Validation Set from all dataset: \", round(len(validationSet)/totalSamples * 100, 2))\n",
    "print(\"Percentage of Test Set from all dataset: \", round(len(testSet)/totalSamples * 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be50540-2c6e-49c8-8f0d-74195188653d",
   "metadata": {},
   "source": [
    "### Maximum and Minimum Lengths of Text from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41fb120b-c5c6-4e90-a905-d0ce690df0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findExtremes(dataSet, textKey):\n",
    "    maxLength, minLength = -math.inf, math.inf\n",
    "    for sample in dataSet:\n",
    "        if(sample[textKey] != None and len(sample[textKey]) > maxLength):\n",
    "            maxLength = len(sample[textKey])\n",
    "        if(sample[textKey] != None and len(sample[textKey]) < minLength):\n",
    "            minLength = len(sample[textKey])\n",
    "    return maxLength, minLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68feef5d-fc63-4345-99be-5159568c5f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Length of Text from Training Set is: 4263\n",
      "Minimum Length of Text from Training Set is: 163\n"
     ]
    }
   ],
   "source": [
    "maxLength, minLength = findExtremes(trainingSet, 'content')\n",
    "print(f\"Maximum Length of Text from Training Set is: {maxLength}\")\n",
    "print(f\"Minimum Length of Text from Training Set is: {minLength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b757df29-c067-47a9-8680-242b27205467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Length of Text from Validation Set is: 2794\n",
      "Minimum Length of Text from Validation Set is: 154\n"
     ]
    }
   ],
   "source": [
    "maxLength, minLength = findExtremes(validationSet, 'content')\n",
    "print(f\"Maximum Length of Text from Validation Set is: {maxLength}\")\n",
    "print(f\"Minimum Length of Text from Validation Set is: {minLength}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a6feaf3-4b36-432b-8fc7-7c3db65b0095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Length of Text from Test Set is: 3479\n",
      "Minimum Length of Text from Test Set is: 167\n"
     ]
    }
   ],
   "source": [
    "maxLength, minLength = findExtremes(testSet, 'content')\n",
    "print(f\"Maximum Length of Text from Test Set is: {maxLength}\")\n",
    "print(f\"Minimum Length of Text from Test Set is: {minLength}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc95d46-1e7e-4afc-882f-a381d609ddbf",
   "metadata": {},
   "source": [
    "### Most Five Frequent Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5955c8c4-2c66-4b49-bf57-4fd4dc7e2517",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a90187e7-2d63-49c3-bd80-25a9796e73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_pipeline_spacy(text):\n",
    "    tokens = []\n",
    "    doc = nlp(text)\n",
    "    for t in doc:\n",
    "        if not t.is_stop and not t.is_punct and not t.is_space:\n",
    "            tokens.append(t.lemma_.lower())\n",
    "    return tokens\n",
    "\n",
    "def makeVocabulary(texts):\n",
    "    tokenizedTexts = [text_pipeline_spacy(text) for text in texts]\n",
    "    allTokens = [token for subList in tokenizedTexts for token in subList]\n",
    "    return Counter(allTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f604f954-be24-404f-aafd-9d2b0ff2f7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 tokens in Training Set for Class 0:  [('john', 16), ('sir', 11), ('henry', 10), ('enclosure', 10), ('james', 9)]\n",
      "Top 5 tokens in Training Set for Class 1:  [('2000', 144), ('7200', 144), ('756', 143), ('gb', 142), ('collection', 31)]\n",
      "Top 5 tokens in Training Set for Class 2:  [('letter', 30), ('include', 15), ('paper', 14), ('list', 12), ('contain', 11)]\n",
      "Top 5 tokens in Training Set for Class 3:  [('seal', 25), ('common', 21), ('mr.', 17), ('college', 17), ('fellow', 16)]\n",
      "Top 5 tokens in Training Set for Class 4:  [('mr.', 18), ('work', 16), ('bill', 15), ('account', 14), ('letter', 14)]\n"
     ]
    }
   ],
   "source": [
    "textsWithLabel0, textsWithLabel1, textsWithLabel2, textsWithLabel3, textsWithLabel4  = [], [], [], [], []\n",
    "for sample in trainingSet:\n",
    "    if(sample['label'] == 0 and sample['content'] != None):\n",
    "        textsWithLabel0.append(sample['content'])\n",
    "    elif(sample['label'] == 1 and sample['content'] != None):\n",
    "        textsWithLabel1.append(sample['content'])\n",
    "    elif(sample['label'] == 2 and sample['content'] != None):\n",
    "        textsWithLabel2.append(sample['content'])\n",
    "    elif(sample['label'] == 3 and sample['content'] != None):\n",
    "        textsWithLabel3.append(sample['content'])\n",
    "    elif(sample['label'] == 4 and sample['content'] != None):\n",
    "        textsWithLabel4.append(sample['content'])\n",
    "\n",
    "print(\"Top 5 tokens in Training Set for Class 0: \", makeVocabulary(textsWithLabel0).most_common(5))\n",
    "print(\"Top 5 tokens in Training Set for Class 1: \", makeVocabulary(textsWithLabel1).most_common(5))\n",
    "print(\"Top 5 tokens in Training Set for Class 2: \", makeVocabulary(textsWithLabel2).most_common(5))\n",
    "print(\"Top 5 tokens in Training Set for Class 3: \", makeVocabulary(textsWithLabel3).most_common(5))\n",
    "print(\"Top 5 tokens in Training Set for Class 4: \", makeVocabulary(textsWithLabel4).most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b3a941-b9ba-40e3-8349-841338a7823b",
   "metadata": {},
   "source": [
    "## Prompting with a Large lLnguage Model (LLM)\n",
    "*<font color='Purple'>Marks - 10</font>*\n",
    "\n",
    "**Task** - <br>\n",
    "A colleague has tried prompting a large language model (Llama-3.1-8B-Instruct) to classify each of the records in the training set. They evaluated three different prompt templates and saved the results to the provided files.\n",
    "- Calculate the accuracy, macro precision, macro recall and macro F1 for each prompt template and comment on the result.\n",
    "  - **Note** - Consider any invalid output from the LLM as predicting a sixth hypothetical class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca641491-45d2-4cc1-a270-1982b1302303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTemplates(promptTemplate, keys):\n",
    "    promptCleaned = {}\n",
    "    key1, key2, key3 = keys\n",
    "    for prompt in promptTemplate:\n",
    "        ID = prompt[key1]\n",
    "        if(ID not in promptCleaned):\n",
    "            try:\n",
    "                promptLabel = int(prompt[key3])\n",
    "            except:\n",
    "                promptLabel = 5\n",
    "            promptCleaned[ID] = {'prompt': prompt[key2], 'label': promptLabel}\n",
    "    return promptCleaned\n",
    "\n",
    "trainPrompts = convertTemplates(trainingSet, ['id', 'content', 'label'])\n",
    "LLM_PromptTemplate1_Dict = convertTemplates(LLM_PromptTemplate1, ['id', 'prompt', 'next_token'])\n",
    "LLM_PromptTemplate2_Dict = convertTemplates(LLM_PromptTemplate2, ['id', 'prompt', 'next_token'])\n",
    "LLM_PromptTemplate3_Dict = convertTemplates(LLM_PromptTemplate3, ['id', 'prompt', 'next_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a08c32e-791b-472a-8915-397acbe7b592",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e29cbe2f-fe0b-422f-8c34-c626dca8031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedData = []\n",
    "for i in range(0, len(trainingSet)):\n",
    "    ID = trainingSet[i]['id']\n",
    "    combinedData.append([\n",
    "        ID,\n",
    "        trainingSet[i]['label'],\n",
    "        LLM_PromptTemplate1_Dict[ID]['label'],\n",
    "        LLM_PromptTemplate2_Dict[ID]['label'],\n",
    "        LLM_PromptTemplate3_Dict[ID]['label'],\n",
    "    ])\n",
    "    \n",
    "# Convert it to NumPy Array\n",
    "combinedData = np.array(combinedData)\n",
    "combinedData = pd.DataFrame(combinedData,\n",
    "    columns = ['ID', 'Train_Labels', 'LLM1_Labels', 'LLM2_Labels', 'LLM3_Labels']\n",
    ")\n",
    "combinedData['Train_Labels'] = combinedData['Train_Labels'].astype(int)\n",
    "combinedData['LLM1_Labels'] = combinedData['LLM1_Labels'].astype(int)\n",
    "combinedData['LLM2_Labels'] = combinedData['LLM2_Labels'].astype(int)\n",
    "combinedData['LLM3_Labels'] = combinedData['LLM3_Labels'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba740d97-3e88-42d9-8a35-dcdb0f7a3f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculateMetrics(trueLabels, predictedLabels):\n",
    "    # Accuracy\n",
    "    uniqueLabels = list(set(trueLabels).union(set(predictedLabels)))\n",
    "    accuracy = accuracy_score(trueLabels, predictedLabels)\n",
    "    \n",
    "    # Precision, Recall, F1 for each class\n",
    "    precision = precision_score(trueLabels, predictedLabels, average = None, labels = uniqueLabels)\n",
    "    recall = recall_score(trueLabels, predictedLabels, average = None, labels = uniqueLabels)\n",
    "    f1Score = f1_score(trueLabels, predictedLabels, average = None, labels = uniqueLabels)\n",
    "    \n",
    "    # Macro averages\n",
    "    macroPrecision = np.mean(precision)\n",
    "    macroRecall = np.mean(recall)\n",
    "    macroF1 = np.mean(f1Score)\n",
    "    \n",
    "    # Confusion Matrix:\n",
    "    if(len(uniqueLabels) == 5):\n",
    "        cmIndex = ['Actual 0', 'Actual 1', 'Actual 2', 'Actual 3', 'Actual 4']\n",
    "        cmColumns = ['Predicted 0', 'Predicted 1', 'Predicted 2', 'Predicted 3', 'Predicted 4']\n",
    "        maIndex = ['0', '1', '2', '3', '4', 'Macro Average']\n",
    "        maColumns = ['Precision', 'Recall', 'F1 Score', 'No of Actual Samples']\n",
    "    elif(len(uniqueLabels) == 6):\n",
    "        cmIndex = ['Actual 0', 'Actual 1', 'Actual 2', 'Actual 3', 'Actual 4', 'Actual 5']\n",
    "        cmColumns = ['Predicted 0', 'Predicted 1', 'Predicted 2', 'Predicted 3', 'Predicted 4', 'Predicted 5']\n",
    "        maIndex = ['0', '1', '2', '3', '4', '5', 'Macro Average']\n",
    "        maColumns = ['Precision', 'Recall', 'F1 Score', 'No of Actual Samples']\n",
    "\n",
    "    print(\"Confusion Matrix: \")\n",
    "    confusionMatrix = pd.DataFrame(confusion_matrix(trueLabels, predictedLabels),\n",
    "        index = cmIndex,\n",
    "        columns = cmColumns\n",
    "    )\n",
    "    print(confusionMatrix.to_string(index = True))\n",
    "\n",
    "    metricMatrix = np.array([[0] * 4 for i in range(0, len(uniqueLabels))], dtype = np.float32)\n",
    "    metricMatrix[:, 0] = np.array(precision)\n",
    "    metricMatrix[:, 1] = np.array(recall)\n",
    "    metricMatrix[:, 2] = np.array(f1Score)\n",
    "    metricMatrix[:, 3] = confusionMatrix.sum(axis = 1)\n",
    "    metricMatrix = np.vstack([metricMatrix, [macroPrecision, macroRecall, macroF1, len(trueLabels)]])\n",
    "    metricMatrix = pd.DataFrame(metricMatrix, \n",
    "         index = maIndex,\n",
    "         columns = maColumns\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    # Print results\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(metricMatrix)\n",
    "    return macroPrecision, macroRecall, macroF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2ca8400-6ca4-4371-9233-dc4ac4d2ffa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Template 1: \n",
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4  Predicted 5\n",
      "Actual 0            0            0            0            0            0           29\n",
      "Actual 1            0            0            0            0            0           33\n",
      "Actual 2            0            0            0            0            0           28\n",
      "Actual 3            0            0            0            0            0           31\n",
      "Actual 4            0            0            0            0            0           29\n",
      "Actual 5            0            0            0            0            0            0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.0\n",
      "               Precision  Recall  F1 Score  No of Actual Samples\n",
      "0                    0.0     0.0       0.0                  29.0\n",
      "1                    0.0     0.0       0.0                  33.0\n",
      "2                    0.0     0.0       0.0                  28.0\n",
      "3                    0.0     0.0       0.0                  31.0\n",
      "4                    0.0     0.0       0.0                  29.0\n",
      "5                    0.0     0.0       0.0                   0.0\n",
      "Macro Average        0.0     0.0       0.0                 150.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikkg\\anaconda3\\envs\\ds_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\nikkg\\anaconda3\\envs\\ds_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"LLM Prompt Template 1: \")\n",
    "LLM1_Precision, LLM1_Recall, LLM1_F1Score = calculateMetrics(combinedData['Train_Labels'], combinedData['LLM1_Labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a4e7046-9392-4876-a99f-bb94704e12d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Template 2: \n",
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0           25            2            1            1            0\n",
      "Actual 1            3           29            1            0            0\n",
      "Actual 2            2            0           22            2            2\n",
      "Actual 3            2            0            1           28            0\n",
      "Actual 4            6            0            0           13           10\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.76\n",
      "               Precision    Recall  F1 Score  No of Actual Samples\n",
      "0               0.657895  0.862069  0.746269                  29.0\n",
      "1               0.935484  0.878788  0.906250                  33.0\n",
      "2               0.880000  0.785714  0.830189                  28.0\n",
      "3               0.636364  0.903226  0.746667                  31.0\n",
      "4               0.833333  0.344828  0.487805                  29.0\n",
      "Macro Average   0.788615  0.754925  0.743436                 150.0\n"
     ]
    }
   ],
   "source": [
    "print(\"LLM Prompt Template 2: \")\n",
    "LLM2_Precision, LLM2_Recall, LLM2_F1Score = calculateMetrics(combinedData['Train_Labels'], combinedData['LLM2_Labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5642119-c88c-4f26-896c-aec9519a0753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Template 3: \n",
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4  Predicted 5\n",
      "Actual 0           24            1            1            1            0            2\n",
      "Actual 1            2           30            1            0            0            0\n",
      "Actual 2            2            1           20            1            1            3\n",
      "Actual 3            5            0            0           26            0            0\n",
      "Actual 4           20            0            1            0            6            2\n",
      "Actual 5            0            0            0            0            0            0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.7066666666666667\n",
      "               Precision    Recall  F1 Score  No of Actual Samples\n",
      "0               0.452830  0.827586  0.585366                  29.0\n",
      "1               0.937500  0.909091  0.923077                  33.0\n",
      "2               0.869565  0.714286  0.784314                  28.0\n",
      "3               0.928571  0.838710  0.881356                  31.0\n",
      "4               0.857143  0.206897  0.333333                  29.0\n",
      "5               0.000000  0.000000  0.000000                   0.0\n",
      "Macro Average   0.674268  0.582762  0.584574                 150.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikkg\\anaconda3\\envs\\ds_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"LLM Prompt Template 3: \")\n",
    "LLM3_Precision, LLM3_Recall, LLM3_F1Score = calculateMetrics(combinedData['Train_Labels'], combinedData['LLM3_Labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bba41a-8414-4a7a-b30f-3cd68b2b8341",
   "metadata": {},
   "source": [
    "### Comment on the Result Obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f60c8-a40e-4730-8c84-1a63287675b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b08dd56-6f30-429c-a271-201ef4ac1aa5",
   "metadata": {},
   "source": [
    "##  Fine-tune a Transformer\n",
    "*<font color='Purple'>Marks - 10</font>*\n",
    "\n",
    "**Task** - <br>\n",
    "- Fine-tune a bert-base-uncased transformer model on the model using the training set.\n",
    "   - **Note**: You should use an **AutoModelForSequenceClassification** and the **HuggingFace** trainer.\n",
    "- Use **8 epochs**, a **learning_rate** of **5e-5** and a batch size of **8**.\n",
    "- Evaluate on the validation set.\n",
    "- Report the per-class precision, recall and F1 score as well as the accuracy, macro precision, macro recall and macro F1 score.\n",
    "\n",
    "**Note** - Dont be surprised with poor performance (see the next question) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf399dff-7134-473d-9faa-709cbd5d1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import pipeline\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import set_seed, AutoModelForSequenceClassification\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ed763aa-edac-4cf3-a4a2-ef637cdf42b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    return tokenizer(examples['content'], padding = \"max_length\", truncation = True, return_tensors = \"pt\", max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a144e5b-7d48-4bed-9424-2cdfaad7b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a Hugging Face Dataset\n",
    "trainDataset = Dataset.from_dict({\n",
    "    'id': [d['id'] for d in trainingSet],\n",
    "    'content': [d['content'] for d in trainingSet],\n",
    "    'label': [d['label'] for d in trainingSet]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c696c3d-b87e-4f72-8504-e80b7c6ba4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a Hugging Face Dataset\n",
    "validationDataset = Dataset.from_dict({\n",
    "    'id': [d['id'] for d in validationSet],\n",
    "    'content': [d['content'] for d in validationSet],\n",
    "    'label': [d['label'] for d in validationSet]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87b01809-9807-4bab-93c8-7b97bfc5e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries to a Hugging Face Dataset\n",
    "testDataset = Dataset.from_dict({\n",
    "    'id': [d['id'] for d in testSet],\n",
    "    'content': [d['content'] for d in testSet],\n",
    "    'label': [d['label'] for d in testSet]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c587ea31-0a9c-45b4-a3e9-b8583a8848ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f53c492c-c95a-422e-a843-ee32b75e314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 5e-5\n",
    "batchSize = 8\n",
    "epochs = 8\n",
    "weightDecay = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdc528b7-e231-4dcc-a9b5-fdfb45ccb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingArguments = TrainingArguments(\n",
    "    output_dir = \"TAD_Coursework\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    num_train_epochs = epochs,\n",
    "    per_device_train_batch_size = batchSize,\n",
    "    per_device_eval_batch_size = batchSize,\n",
    "    learning_rate = learningRate,\n",
    "    weight_decay = weightDecay,\n",
    "    report_to = 'none',\n",
    "    logging_steps = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d47ed52-480c-41bc-aa5c-496697eb1950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0776067affb646758dbf707af378c927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c256d8516c4f4fba8b06c400d36396e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataCollator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = trainingArguments,\n",
    "    train_dataset = trainDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True),\n",
    "    eval_dataset = validationDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True),\n",
    "    data_collator = dataCollator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "02273fa5-7ff9-4aa6-a767-d1cef545cffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 01:07, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.611000</td>\n",
       "      <td>1.718849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.918974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.612900</td>\n",
       "      <td>2.296982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.223800</td>\n",
       "      <td>2.811447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.078200</td>\n",
       "      <td>3.539253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>3.947955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>4.086780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>4.138451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=152, training_loss=0.44828743050119985, metrics={'train_runtime': 67.7463, 'train_samples_per_second': 17.713, 'train_steps_per_second': 2.244, 'total_flos': 315741770956800.0, 'train_loss': 0.44828743050119985, 'epoch': 8.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e271da35-6666-4aa9-943e-f748ef3cf00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb07f152fe249c28f2b42098f453aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizedValidationDataset = validationDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "predictions, label_ids, metrics = trainer.predict(tokenizedValidationDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaec244-3582-49b3-b74d-ef45ce899c9b",
   "metadata": {},
   "source": [
    "## A Problem with the Validation Set\n",
    "*<font color='Purple'>Marks - 5</font>*\n",
    "\n",
    "**Task** - <br>\n",
    "There is an issue with the validation set which causes poor performance.\n",
    "- Provide the confusion matrix.\n",
    "- Describe the problem, how you identified it and how you fixed it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "33aacbc4-a7e4-4971-97b0-7b3b9d71a002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set: \n",
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0            1            0            0            0            7\n",
      "Actual 1            0            0            3           10            0\n",
      "Actual 2            0            0           11            0            0\n",
      "Actual 3            0            9            1            0            0\n",
      "Actual 4            7            0            0            1            0\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.24\n",
      "               Precision  Recall  F1 Score  No of Actual Samples\n",
      "0               0.125000   0.125  0.125000                   8.0\n",
      "1               0.000000   0.000  0.000000                  13.0\n",
      "2               0.733333   1.000  0.846154                  11.0\n",
      "3               0.000000   0.000  0.000000                  10.0\n",
      "4               0.000000   0.000  0.000000                   8.0\n",
      "Macro Average   0.171667   0.225  0.194231                  50.0\n"
     ]
    }
   ],
   "source": [
    "predictedValidationLabels = np.argmax(predictions, axis = 1)\n",
    "print(\"Validation Set: \")\n",
    "Val_Precision, Val_Recall, Val_F1Score = calculateMetrics(tokenizedValidationDataset['label'], predictedValidationLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b5eee-37c1-4047-930a-ea63c09e0b11",
   "metadata": {},
   "source": [
    "Just looking at the **Confusion Matrix**, one can assume the samples are accidently labeled wrong. Since,\n",
    "- All samples with Label 0 are Predicted as Label 4 and vice versa.\n",
    "- Similarly, all the samples with Label 1 are Predicted as Label 3 and vice versa.\n",
    "\n",
    "Let's check our suspicioun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "faca6170-528f-4e4a-aee8-c3ceef2d7da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 is supposed to be for Institution: National Maritime Museum\n",
      "Label 4 is supposed to be for Institution: Shakespeare Birthplace Trust\n",
      "\n",
      "Memorandum 'Item to knowe whether hyt be lawfull fr ye xij men that were sworne to gyve verdyt on hyt or no seyng the sheryf and bayles are changed. Also whether hyt be lawfull for the new shyryfe & bayles to [?] the same men syng they knowe not who they are but by the ynformatyon of the partyes'\n",
      "\n",
      "\n",
      "Thomas Garth, Philip Philips, G.Payne to [Edward Earl of Orford] Letter, duplicate copy. Having received letters of attorney they now enclose their accounts showing the balance due to Col. Francis Russell's executors. He will be surprised at the amount of the debts of which they, who were daily with the Colonel, knew little though they have paid only what debts could be proved. They explain the plan adopted for getting a certain 300 from the government, and mention some matters relating to infringement of the secretary's rights, and papers which are being sent to Mr. Bridges to be shown to Mr. Moreley. Mention is made of Mrs. Coates, Mr. Langley, Mr. Foullerton.\n",
      "\n",
      "\n",
      "A draft of some of the Chamberlains' accounts apparently for the year 1588. There are payments for washing the walls about the chapel, claret and sack for the clerk of the market, a pottill of wine that Sur Foucke Grevill had, wyne and suger when Sur Thomas Lewsi sat in commission for the tippelers, &c. Four pages, of which the last may possibly refer to the year 1587\n",
      "\n",
      "\n",
      "APPRENTICESHIP INDENTURES. Calendar of Apprenticeship papers and Indentures including those from Shipston-on-Stour The main collection of the above indentures from 1606 to 1843 are among the Corporation archives and others among their miscellaneous documents. Others again will be found in the records of the Borough and County Magistrates. The present series is taken from the old 'Miscellanea' and with it has been incorporated the indentures discovered in Mr. Basil Hancocks office in Shipston.\n",
      "\n",
      "\n",
      "Draft case for the Opinion of Counsel as to whether Aldermen are eligible for election as Chamberlains or whether capital burgesses only, although since 1780 Aldermen have for the most part filled this office. Also whether Mayor can insist upon Chamberlain delivering to him such Records as he finds it necessary to consult.\n",
      "\n",
      "\n",
      "Mr. Botte his bill for worke don for the Chamber, for ironwork for the rails att the communing tabell in the Chapell, for cramps for the Markett Hall, & c\n",
      "\n",
      "\n",
      "A minute of orders for the generall uniformitie of all sorts of armes both for horse and foote, and for the perfecting of musters, and for the exercise of horse, according to the discipline that is used at this daie\n",
      "\n",
      "\n",
      "Samuel Ogle [a commissioner of the revenue for Ireland] to ? Copy of letter. The rents accruing to Michaelmas last will balance Lady Dorchester's arrears of pension, but the money will not come in for some time. The money is collected by the commissioners' own collectors for very good reason, but they should be obliged to clear all at the next accounts. The lord treasurer should be asked to direct that particular care be taken in collecting all arrears, or Lady Russell's affair may go on heavily. There should be no difficulty after this.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 0 is supposed to be for Institution: National Maritime Museum\")\n",
    "print(\"Label 4 is supposed to be for Institution: Shakespeare Birthplace Trust\\n\")\n",
    "for i in range(0, len(validationSet)):\n",
    "    if(validationSet[i]['label'] == 0):\n",
    "        print(validationSet[i]['content'])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "706be9bf-1d0c-4ada-8204-e42e6ad4deb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 4 is supposed to be for Institution: Shakespeare Birthplace Trust\n",
      "Label 0 is supposed to be for Institution: National Maritime Museum\n",
      "\n",
      "Copy letter from I Mayo, 31 Queen Ann Street [to Miss Payne, sister of John Payne]. Am sending the account of the appearances in the brain of your brother as they presented themselves on examination by Mr C Bell and myself. It confirms my first opinion that the high state of delirium depended on inflammatory action of the covering of the brain. One frequent cause of this is too great a strh of the mind in concerns of deep calculation, especially when done over lengthy periods and accompanied by anxiety. Consider it not to be a family, but an accidental, disease. Enclosure.\n",
      "\n",
      "\n",
      "M.James, Curate. Certifies that John son of John and Mary Barton was baptized July 3rd 1740 as appears in the register book of the parish of Chatham in the county of Kent. Enclosure.\n",
      "\n",
      "\n",
      "List of men who signed for their money from Captain Oxley of the Lively: John Peet, Ernest Conway, Henry Richardson, John Lonergan, William Watson, Thomas Horley, John Barry, Thomas Larsen, Michael Fitzgerald and John Shelley. Enclosure.\n",
      "\n",
      "\n",
      "John Barrow. Has interviewed Benjamin Horner who keeps a public house called the Anchor and Vine at the corner of Buckingham Court, adjacent to the Admiralty premises, with a view to treating for the property. Have also written to Mr Calvert, making an offer for the residue of his lease.\n",
      "\n",
      "\n",
      "Slain Bounty - list of claimants who have recently applied: Widows Mary Ann Rooney of James, and Mary Screech of Charles, Seamen of the Genoa, killed at Navarino, and Johannah Sparrow of Richard, Seaman of the Brisk, killed at Navarino and Mary Ann Woolcock of John Seaman of the Sybille, in action with pirates in 1826. Enclosure.\n",
      "\n",
      "\n",
      "William Stevens, Clerk of the Petitions. Reports on petition being presented by the wife of Peter Lortia with a letter of attorney from Joseph Parker to Peter Lortia sent on shore sick Enclosure.\n",
      "\n",
      "\n",
      "Captain Richard Jasper. Extracts from the General examinations of the principal persons by Captain Alexander Innes, by order of the Governor of Havana. Don Juan de Mora, Chief Master Caulker, Don Francisco De Leon Galero, Commissary of the Arsenal for the Royal Company; Joseph Ferrar, a Master Smith, Ignacio Joseph de Esperon, Chief Rigging Master and Joseph Nicholas De Herara. Enclosure.\n",
      "\n",
      "\n",
      "George Elliot. The 60-day and other bills issued in 1830 have been included in the account of receipts and expenditures previously conveyed and a substantial balance remains with the Treasury.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 4 is supposed to be for Institution: Shakespeare Birthplace Trust\")\n",
    "print(\"Label 0 is supposed to be for Institution: National Maritime Museum\\n\")\n",
    "for i in range(0, len(validationSet)):\n",
    "    if(validationSet[i]['label'] == 4):\n",
    "        print(validationSet[i]['content'])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8c0eca49-ec06-44cf-b7be-375ec51ee46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 1 is supposed to be for Institution: National Railway Museum\n",
      "Label 3 is supposed to be for Institution: Royal College of Physicians of London\n",
      "\n",
      "Notebook of William Fleming, Bedell. MS notebook with \"Notebook. W. Fleming (Bedell) on cover. Some entries in hand of William Fleming, others in different hands, covering variety of subjects. Mainly copies of correspondence, minutes etc. dates varying between 1834 and 1916, but book itself probably covers the years William & Fleming was Bedell - 1893-1923. Entries include Dr. Watson's Presidential Address 26 March, 1866 INDEX REFERENCES: Watson (Sir Thomas), 1792-1882\n",
      "\n",
      "\n",
      "Mr Abernethy's Surgical Essays delivered MDCCCXX.MDCCCXXI (Quotation from Serenus Sammonicus). PP. 1-241 probably in hand of Charles Witt. PP. 242-386 slightly different hand. P. 615 in handwriting of Dr George Witt. Begins p. 1: \"On fevers attending injuries. Medicine is in itself one and indivisible; it is a science which has for its object the cure of diseases in general.\" Ends p. 241: (lect XXX). \"I have here given a mere outline of surgery, as I know it to be impossible to teach it by lectures - facts and fine reasonings may be rendered so mawkish by dilution as to become tiresome and disgusting - I see no good in lecturing to people when they are asleep & after all the utmost extent that lectures can do, is to send us out to practice with minds prepared to meditate and to observe. I consider myself in the light of a topographer, as one who has travelled & is describing the country to others, who does not mention every hedge, gate & puddle where we may dirty our boots, but who points out where good accomodation may be had & where we may rest in peace.\" Followed by: \"Surgical extracts from Mr Abernethy's anatomical lectures; delivered 1820.\" Begins p. 242: \"On the blood. In considering the subject of animal matter, I shall follow Haller's division.\" Ends p. 386: \"In the horse, the carotid arteries pursue a very tortuous course down the neck, and before they enter the skull.\" Followed by about a hundred blank leaves. At the end of the volume is a \"General Index\" (5 pp.) and an \"Index to Mr Abernethy's surgical lectures\" (p. 1) The text differs considerably from that of any of the published lectures. On the last page is the following declaration: \"We the undersigned who have been permitted to take a copy of the foregoing Lectures of the late Mr Abernethy hereby promise and agree in the most unequivocal manner that we will never allow any person to take a copy without first obtaining the sanction of Dr Witt, in testimony whereof we have hereunto subscribed our respective names.\" Signed in 1835 by Wm. Thurnall, Matthew Inman, Richard Henry Meade, James Edman Beveridge, Goerge Dixon Hedley, Charles Longuet Higgins, Charles Frederick Edwards and Thomas Mitchell, and in 1843 by W.R. Nusham. The declaration is in Dr George Witt's handwriting, probably Charles' brother. Note: Charles Witt studied at St Bartholomew's Hospital, qualified MRCS in 1821 and became an extra-licentiate of the RCP in 1841. He was House Surgeon to the General Infirmary, Northampton, and died 23 April 1883, aged 86. George Witt graduated M.D. at Leyden in 1828, and was for some time a surgeon in the service of the Hon. East India Co. On his return to England, he became in 1845 an extra-licentiate of the RCP, and obtained the post of physician to the Bedford General Infirmary.\n",
      "\n",
      "\n",
      "COMMITTEES. Committee of Reference R.C.P. Report of the proceedings of the Committee of Reference (Typescript. With some printed documents). The Committee of Reference was appointed by the Royal Colleges of Physicians and Surgeons \"to advise the Government Departments concerned through the Central Medical War Committee, on any case affecting the several Metropolitan Hospitals and Medical Schools during the war in respect of medical men on their staff with regard to whom the question arises as to whether a particular individual is indispensable or would suffer excessive personal hardship if required to enter Military Service: and further advise on the case of any other medical men in England or Wales in respect of whom the Central Tribunal under the Military Service Act thinks it desirable that this Advisory Committee should be consulted.\"\n",
      "\n",
      "\n",
      "William Harvey 1578-1657 F. 1607. By Peter Scheemakers Head to left, eyes (incised) looking in the same direction; long hair, moustache and pointed beard; falling collar, gown over tight-buttoned doublet; circular socle. Incised on the back: GUL. HARVEIUS M.D.\n",
      "\n",
      "\n",
      "Cash book. PHYSICAL DESCRIPTION 458 [Followed by 7pp. of receipted bills] 32 x 22 cms. Vellum with clasps. ORIGIN AND PROVENANCE Coll. Coll. C 06.03 Note at end of numbered pages: \"The bills and receipts which follow were found loose in the cellar in 1950 and incorporated here by J.J. Keevil.\" INDEX REFERENCES: Keevil (John Joyce), 1901-1958\n",
      "\n",
      "\n",
      "CHARLES II. EXEMPLIFICATION. Letters Patent (Inspeximus) exemplifying the Statute 14 & 15 Henry VIII (The priviledges & Authority of Physicians in London) Recital of the Statute 14 & 15 Henry VIII Cap.5, etc. NOS antem tenorem p[re] missor[um] p[re]d[i]c[t]or[um] ad requisic[i] & [n] em Franci ?Berry de London gen. DUXIMUS exemplificand. p[re] p[re]sentes. IN cujus rei Testimoniu[m] has l[ite]ras n[ost]ras fieri fecimus Patentes. Teste meip[s]p apud Westm.vicesimo. (here damaged) Junii Anno Regni nostri tricesimo quinto. GRIM[STON]\n",
      "\n",
      "\n",
      "Case for Counsel, Sir Samuel Shepherd, C.Warren & John Lens and their opinion on the question whether the Censors of the College of Physicians are empowered to examine the Files of such Apothecaries as they may believe to possess prescriptions of physicians guilty of Mala Praxis. And also of unlicensed practitioners whose practice may be presumed to be bad as they have never been examined as required by the Charter and Act of Parliament. And whether they are empowered to demand the delivery of such prescriptions either to themselves or to the Beadle of the College\n",
      "\n",
      "\n",
      "Treatise on Urines etc. PHYSICAL DESCRIPTION 137 pp 28 x 19 cms. Parchment. ORIGIN AND PROVENANCE p. 159 (1928) Catalogued by Mrs. D. Waley Singer. BIBLIOGRAPHY N.R. Ker. Medieval Manuscripts in British Libraries. Oxford, 1969. P. 206 No. 356 In middle English\n",
      "\n",
      "\n",
      "HMS Achilles Surgeons journal. A handwritten medical log detailing the cases of the men on the HMS Achilles from 17 May 1877 to 31 December 1877 on the way to and from the Mediterranean/Turkey  including (as listed in the index) cases of abscesses, aneurisms, Brights disease, Bubo symp. (Bubonic symptoms?), Catarrh, contusions, cynanche, cystitis, debility, diarrhoea, enteric fever, enteritis, epilepsy, fracture, homophlysis, heart disease, hydrocele, impetigo, incontinence of urine, orchitis, phthisis, pneumonia, rheumatism, scrofula, spinal curvature, splenitis, and wounds. There is a quote from Virgil, erunt etiam altera bella, atque iterum ad trojam magnus mittetur achilles which means New wars too shall arise, and great Achilles will be sent again to Troy. Interesting cases include that of William Gaskells contusion and loss of limb function, John Julians wounded arm, William Warrens rheumatism, Thomas Clares enteric fever, Richard Pitchers splenilis, and Charles Bracknells aneurisms. The back of the journal holds a handwritten report on Bashika Bay and a map. The journal includes a photo of the Royal Naval Hospital in Malta, meteorological reports for all months the expedition occurred, a map of the Dardanelles and the Troad, and an appendix on Bashika Bay.\n",
      "\n",
      "\n",
      "The morbid anatomy of some of the most important parts of the human body.\" [Printed]. The morbid anatomy of some of the most important parts of the human body. 1793. Interleaved copy, bound in two volumes, with additions and alterations in the handwriting of the author\n",
      "\n",
      "\n",
      "Major T. G. Tulloch. Typescript letter to Sir William Willcox. Stating that he did not think salt was a necessity for the blood and there was no justification for adding it artificially; suggesting it would be better for miners to replace loss of salt through sweating by drinking KCL water rather than salted water, and asking him to suggest this to the minister of mines after he was ready to supply refined KCL in quantity; and discussing other matters relating to the production of potash table salt and Sir William's role as medical adviser.\n",
      "\n",
      "\n",
      "Copy of typescript letter enclosing reports on the post-mortem on Andrey Juschtschinsky. Stating the medical affidavits afforded a basis for the absurd charge of ritual murder; that they were obtaining a scientific criticism; that the German medical men agreed the whole fabric of the charge (of ritual murder) was unscientific; and that the Russians wanted similar reports from England, France and Austria.\n",
      "\n",
      "\n",
      "Drawing by Joseph Perry. With verso note: \"A large fibrous tumour covered with the lining membrane and a layer of the muscular substance of the uterus, and note of publication in Robert Lees' Practical Observations on the Diseases of the uterus, Churchill, 1849. Plate VI. Fig. 4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 1 is supposed to be for Institution: National Railway Museum\")\n",
    "print(\"Label 3 is supposed to be for Institution: Royal College of Physicians of London\\n\")\n",
    "for i in range(0, len(validationSet)):\n",
    "    if(validationSet[i]['label'] == 1):\n",
    "        print(validationSet[i]['content'])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89798c13-2e59-4f9b-9272-c86832c16771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 3 is supposed to be for Institution: Royal College of Physicians of London\n",
      "Label 1 is supposed to be for Institution: National Railway Museum\n",
      "\n",
      "Class 2-2-6-0, DETAILS FOR BLOW-DOWN Class 2-2-6-2T, DETAILS FOR BLOW-DOWN Class 4-2-6-0, DETAILS FOR BLOW-DOWN Class 5-4-6-0, DETAILS FOR BLOW-DOWN Class 6-4-6-2, DETAILS FOR BLOW-DOWN Class 7-4-6-2, DETAILS FOR BLOW-DOWN Class 8-4-6-2, DETAILS FOR BLOW-DOWN\n",
      "\n",
      "\n",
      "The British Railways (Western Region) signalling collection. This collection, acquired by the NRM in 1997, features signalling on British Railways Western Region, the lines previously served by the GWR. Covering Wales, the West Country and the Midlands, the photographs depict permanent way equipment, staff at work, lifting barriers, level crossings, marshalling yards, stations, control panels, signal box interiors, relay rooms and the signals themselves. These themes are repeated in the 35mm slides and lanternslides, but images of circuit diagrams feature more prominently, as do close-ups of the workings of signals. A handwritten register gives details of the subject and negative number. Prints are ordered by location and listed in the registers by place.\n",
      "\n",
      "\n",
      "Sealink ferry services publicity and advertising photographs. This collection covers the operation of British Railways' ferry services, with particular emphasis on the period from 1965 to 1978. There are photographs of docks, freight traffic, container handling and passengers, mainly taken for publicity or advertising purposes. Ships featured include the 'Antrim Princess', 'Sarnia', 'Amsterdam', 'Vortigern', 'Avalon', 'Anselm', 'Maid of Kent', 'Hengist'. 'Lord Warden' and 'Caledonia Princess'. A small number of images feature earlier railway ferry services, and there are also photographs of the LNER's east coast ports, fish being landed at Grimsby and ferries that operated in the 1930s. The prints are listed by vessel, location or date. The negatives are also listed, with brief descriptions of subject and date where known. However, negatives for the majority of images in this collection are not held by the NRM.\n",
      "\n",
      "\n",
      "The Andrews collection of prints showing the Mobile Testing Plant of the London, Midland and Scottish Railway. This collection primarily consists of prints showing the operations of the Mobile Testing Plant of the LMS, which Andrews had played an instrumental role in establishing. The photographs include trials on vehicles and the permanent way. Most of the prints are captioned.\n",
      "\n",
      "\n",
      "The Ian Allan Ltd collection of railway photographs. This collection includes the work of Stanley Rhodes, H Gordon Tidey, W H Whitworth and E R Wethersett whilst there are also sections which incorporate images by other photographers, some unidentified.\n",
      "\n",
      "\n",
      "Gleneagles Hotel photographic collection. The Gleneagles Hotel collection comprises 6 x 4 and 8 x 6 ins glass plate and film negatives, and black and white transparencies, all featuring golf at the Gleneagles Hotel course. The collection includes publicity photographs of the course, posed views of golfers, together with photographs of major competitions in progress.\n",
      "\n",
      "\n",
      "The Wolverton Carriage & Wagon Works photographic collections. There are two main Wolverton collections. The first, comprising some 850 negatives, primarily covers the LNWR period, with images of newly completed carriages, including Travelling Post Offices, kitchen cars and First World War ambulance trains. Road vehicles also feature, with views of buses operated by the LNWR. The second component, about 485 negatives, is similar, but dates from a slightly later period, comprising posed photographs of LMS carriages and wagons at the works. The LNWR component is listed and a catalogue was also published by the Railprint Joint Venture, which marketed the collection in the late 1970s and early 1980s. Photographs once sold by Railprint appear on their list P14. The LMS collection is listed in a handwritten register, with reference prints available for consultation in the Reading Room.\n",
      "\n",
      "\n",
      "Hugh Hughes' collection of photographs covering the electrification of lines in southern England. The Hugh Hughes collection covers the electrification of lines in southern England and features prints of LBSCR converted steam stock, Waterloo & City, Southern and British Railways electric trains. These originate from a variety of sources, including the Topical Press and the carriage builders, Metro. There are also documents relating to the Southern Railway and its pre-grouping constituents including timetables, drawings and notices.\n",
      "\n",
      "\n",
      "Class 4-2-6-4T, HANDBRAKE & WATER PICK-UP OPERATING Class 4/BR1/1H-4250gal, HANDBRAKE & WATER PICK-UP OPERATING Class 4/BR2/2A-3500gal, HANDBRAKE & WATER PICK-UP OPERATING Class 4/BR1F-5625gal, HANDBRAKE & WATER PICK-UP OPERATING Class 4/BR1A/1G-5000gal, HANDBRAKE & WATER PICK-UP OPERATING Class 4/BR1B-1E-4725gal, HANDBRAKE & WATER PICK-UP OPERATING\n",
      "\n",
      "\n",
      "Frame stay between trailing and intermediate wheels; frame stay between leading and driving wheels; bogie centre pin and frame stay; bogie life guard EO 359, 364, 365, GB 756 2000-7200/O2; Darlington - GB 756 2000-7200/O2 1940 deleted; GB 756 2000-7200/D49, GB 756 2000-7200/A10, GB 756 2000-7200/A1, GB 756 2000-7200/A3, GB 756 2000-7200/A4\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Label 3 is supposed to be for Institution: Royal College of Physicians of London\")\n",
    "print(\"Label 1 is supposed to be for Institution: National Railway Museum\\n\")\n",
    "for i in range(0, len(validationSet)):\n",
    "    if(validationSet[i]['label'] == 3):\n",
    "        print(validationSet[i]['content'])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2435dd-d3ce-4634-b890-84d4462e04d4",
   "metadata": {},
   "source": [
    "Well, **Solution** - \n",
    "- Interchange the labels between Class 1 and Class 3, as well Class 0 and Class 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "15cc1262-91a0-4a6d-a702-e2de9da98a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d52c6ad7d2b48de886226e6c5d29867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(0, len(validationSet)):\n",
    "    if(validationSet[i]['label'] == 0):\n",
    "        validationSet[i]['label'] = 4\n",
    "    elif(validationSet[i]['label'] == 4):\n",
    "        validationSet[i]['label'] = 0\n",
    "    elif(validationSet[i]['label'] == 1):\n",
    "        validationSet[i]['label'] = 3\n",
    "    elif(validationSet[i]['label'] == 3):\n",
    "        validationSet[i]['label'] = 1\n",
    "\n",
    "# Convert the list of dictionaries to a Hugging Face Dataset\n",
    "validationDataset = Dataset.from_dict({\n",
    "    'id': [d['id'] for d in validationSet],\n",
    "    'content': [d['content'] for d in validationSet],\n",
    "    'label': [d['label'] for d in validationSet]\n",
    "})\n",
    "\n",
    "tokenizedValidationDataset = validationDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5234270-2d06-495e-92be-219170dd0a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, label_ids, metrics = trainer.predict(tokenizedValidationDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c1facf3-96f3-43b4-89c3-7042190dec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Set: \n",
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0            7            0            0            1            0\n",
      "Actual 1            0            9            1            0            0\n",
      "Actual 2            0            0           11            0            0\n",
      "Actual 3            0            0            3           10            0\n",
      "Actual 4            1            0            0            0            7\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.88\n",
      "               Precision    Recall  F1 Score  No of Actual Samples\n",
      "0               0.875000  0.875000  0.875000                   8.0\n",
      "1               1.000000  0.900000  0.947368                  10.0\n",
      "2               0.733333  1.000000  0.846154                  11.0\n",
      "3               0.909091  0.769231  0.833333                  13.0\n",
      "4               1.000000  0.875000  0.933333                   8.0\n",
      "Macro Average   0.903485  0.883846  0.887038                  50.0\n"
     ]
    }
   ],
   "source": [
    "predictedValidationLabels = np.argmax(predictions, axis = 1)\n",
    "print(\"Validation Set: \")\n",
    "Val_Precision, Val_Recall, Val_F1Score = calculateMetrics(tokenizedValidationDataset['label'], predictedValidationLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee534009-8b07-47ce-9b3f-8076faa9ab7c",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "*<font color='Purple'>Marks - 12</font>*\n",
    "\n",
    "**Task** - <br>\n",
    "Train and evaluate several fine-tuned transformer models using the corrected training and validation sets.\n",
    "- Try the four base models listed below. Base models to try:\n",
    "  - bert-base-uncased\n",
    "  - roberta-base\n",
    "  - distilbert-base-uncased\n",
    "  - microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\n",
    "- Use 8 epochs, a learning_rate of 5e-5 and a batch size of 8.\n",
    "\n",
    "**Note** - Ideally, you would try different base_models/learning_rates/batch_sizes/etc, but we will limit this to evaluating four different base models \n",
    "and keep the remaining hyperparameters static. \n",
    "  \n",
    "This time, we want the best model found during the training process for each base model and to save it for the final analysis. <br>\n",
    "For example, if the model after 3 epochs is the best performing on the validation set (by macro-F1), we want to keep that. You should investigate the load_best_model_at_end parameter for the Trainer (which does require other parameters). <br>\n",
    "- Evaluate each fine-tuned model on the validation set.\n",
    "- Report the per-class precision, recall and F1 score as well as the accuracy, macro precision, macro recall and macro F1 score.\n",
    "- Comment on the performance of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f836ebc-6c82-46a4-82a8-056592eff858",
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 5e-5\n",
    "batchSize = 8\n",
    "epochs = 8\n",
    "weightDecay = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2678a791-647d-4e23-b9cf-ca02f11b983b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa485956eea490c9d7d9ee5a795545b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffefcfa1a45c425aa1b1a6b6cae88f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 01:20, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.559500</td>\n",
       "      <td>1.191282</td>\n",
       "      <td>0.572054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.000200</td>\n",
       "      <td>0.719879</td>\n",
       "      <td>0.806429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.498100</td>\n",
       "      <td>0.434720</td>\n",
       "      <td>0.909333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.200800</td>\n",
       "      <td>0.267449</td>\n",
       "      <td>0.925897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.153529</td>\n",
       "      <td>0.962586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.182260</td>\n",
       "      <td>0.925897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.151345</td>\n",
       "      <td>0.962586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.157147</td>\n",
       "      <td>0.962586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aaf29c3672d40248085e52215cae9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2444cf9c5945bebcf224cee08fbfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 01:23, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.620000</td>\n",
       "      <td>1.529926</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.407300</td>\n",
       "      <td>0.834338</td>\n",
       "      <td>0.709938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.460690</td>\n",
       "      <td>0.833778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.308077</td>\n",
       "      <td>0.884245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.397409</td>\n",
       "      <td>0.882167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.410625</td>\n",
       "      <td>0.884245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.464710</td>\n",
       "      <td>0.863120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.454153</td>\n",
       "      <td>0.863120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52160196b71047cf8a8db078aa947187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b100624751a044108e13c7a000cb74b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 00:43, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.568800</td>\n",
       "      <td>1.175572</td>\n",
       "      <td>0.702613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.985900</td>\n",
       "      <td>0.722851</td>\n",
       "      <td>0.842136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.407500</td>\n",
       "      <td>0.392156</td>\n",
       "      <td>0.935037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>0.226395</td>\n",
       "      <td>0.933034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.214168</td>\n",
       "      <td>0.960037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.199114</td>\n",
       "      <td>0.960037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.190731</td>\n",
       "      <td>0.960037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.190345</td>\n",
       "      <td>0.960037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b4cc50088d47268b2ddc46286a10ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec4b0a6fdf740bfa507e772303a9a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 01:19, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.569600</td>\n",
       "      <td>1.155138</td>\n",
       "      <td>0.634872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.645296</td>\n",
       "      <td>0.814781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.478603</td>\n",
       "      <td>0.842018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>0.505737</td>\n",
       "      <td>0.819601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.517671</td>\n",
       "      <td>0.835683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.529632</td>\n",
       "      <td>0.808515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.476840</td>\n",
       "      <td>0.855498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.511675</td>\n",
       "      <td>0.829505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_macro_f1(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis = -1)  # Convert logits to predicted labels\n",
    "    f1 = f1_score(labels, predictions, average = 'macro')\n",
    "    return {'macro_f1': f1}\n",
    "\n",
    "trainingModels = ['bert-base-uncased', 'roberta-base', 'distilbert-base-uncased', 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract']\n",
    "bestModelCheckPoints = []\n",
    "for train_model in trainingModels:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(train_model)\n",
    "    tokenizedTrainingDataset = trainDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "    tokenizedValidationDataset = validationDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(train_model, num_labels = 5)\n",
    "    trainingArguments = TrainingArguments(\n",
    "        output_dir = f\"Model_{train_model.upper()}\",\n",
    "        eval_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        num_train_epochs = epochs,\n",
    "        per_device_train_batch_size = batchSize,\n",
    "        per_device_eval_batch_size = batchSize,\n",
    "        learning_rate = learningRate,\n",
    "        weight_decay = weightDecay,\n",
    "        report_to = 'none',\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"macro_f1\",\n",
    "        greater_is_better = True,\n",
    "        logging_steps = 10\n",
    "    )\n",
    "\n",
    "    dataCollator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = trainingArguments,\n",
    "        train_dataset = tokenizedTrainingDataset,\n",
    "        eval_dataset = tokenizedValidationDataset,\n",
    "        data_collator = dataCollator,\n",
    "        compute_metrics = compute_macro_f1,\n",
    "    )\n",
    "    trainer.train()\n",
    "    best_model_path = trainer.state.best_model_checkpoint\n",
    "    bestModelCheckPoints.append(best_model_path)\n",
    "    tokenizer.save_pretrained(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a8dfe06-56af-42d9-8b06-920d4c787c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path for Best Model Checkpoints: \n",
      "- Model_BERT-BASE-UNCASED\\checkpoint-95\n",
      "- Model_ROBERTA-BASE\\checkpoint-76\n",
      "- Model_DISTILBERT-BASE-UNCASED\\checkpoint-95\n",
      "- Model_MICROSOFT/BIOMEDNLP-BIOMEDBERT-BASE-UNCASED-ABSTRACT\\checkpoint-133\n"
     ]
    }
   ],
   "source": [
    "print(\"Path for Best Model Checkpoints: \")\n",
    "print(f\"- {bestModelCheckPoints[0]}\")\n",
    "print(f\"- {bestModelCheckPoints[1]}\")\n",
    "print(f\"- {bestModelCheckPoints[2]}\")\n",
    "print(f\"- {bestModelCheckPoints[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b15329d-bb72-430d-ab70-575467342599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Using Bert-Base-Uncased Model: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a94208d7f6d46f1ba0630487711aa41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0            7            0            0            1            0\n",
      "Actual 1            0           10            0            0            0\n",
      "Actual 2            0            0           11            0            0\n",
      "Actual 3            0            0            1           12            0\n",
      "Actual 4            0            0            0            0            8\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.96\n",
      "               Precision    Recall  F1 Score  No of Actual Samples\n",
      "0               1.000000  0.875000  0.933333                   8.0\n",
      "1               1.000000  1.000000  1.000000                  10.0\n",
      "2               0.916667  1.000000  0.956522                  11.0\n",
      "3               0.923077  0.923077  0.923077                  13.0\n",
      "4               1.000000  1.000000  1.000000                   8.0\n",
      "Macro Average   0.967949  0.959615  0.962586                  50.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting Using Bert-Base-Uncased Model on Validation Set: \")\n",
    "\n",
    "model_checkpoint = bestModelCheckPoints[0]\n",
    "# Load the best model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizedValidationDataset = validationDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define the Trainer with the best model\n",
    "trainer = Trainer(\n",
    "    model = model\n",
    ")\n",
    "\n",
    "predictions, label_ids, metrics = trainer.predict(tokenizedValidationDataset)\n",
    "predictedLabels_ForBertBaseUncased = np.argmax(predictions, axis = 1)\n",
    "BertBase_Precision, BertBase_Recall, BertBase_F1Score = calculateMetrics(tokenizedValidationDataset['label'], predictedLabels_ForBertBaseUncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d4d1db6c-2f70-4798-a3f2-bb9a058c0a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Using Roberta-Base Model: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5479d0b9e841fd903d9baf7ed4a78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0            6            0            0            1            1\n",
      "Actual 1            0           10            0            0            0\n",
      "Actual 2            1            0           10            0            0\n",
      "Actual 3            0            0            0           13            0\n",
      "Actual 4            2            0            0            0            6\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.9\n",
      "               Precision    Recall  F1 Score  No of Actual Samples\n",
      "0               0.666667  0.750000  0.705882                   8.0\n",
      "1               1.000000  1.000000  1.000000                  10.0\n",
      "2               1.000000  0.909091  0.952381                  11.0\n",
      "3               0.928571  1.000000  0.962963                  13.0\n",
      "4               0.857143  0.750000  0.800000                   8.0\n",
      "Macro Average   0.890476  0.881818  0.884245                  50.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Predictiing Using Roberta-Base Model on Validation Set: \")\n",
    "\n",
    "model_checkpoint = bestModelCheckPoints[1]\n",
    "# Load the best model\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "tokenizedValidationDataset = validationDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define the Trainer with the best model\n",
    "trainer = Trainer(\n",
    "    model = model\n",
    ")\n",
    "\n",
    "predictions, label_ids, metrics = trainer.predict(tokenizedValidationDataset)\n",
    "predictedLabels_ForRobertaBaseUncased = np.argmax(predictions, axis = 1)\n",
    "RobertaBase_Precision, RobertaBase_Recall, RobertaBase_F1Score = calculateMetrics(tokenizedValidationDataset['label'], predictedLabels_ForRobertaBaseUncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5e13f278-6731-429b-b580-e04893813e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Using Distilbert-Base-Uncased Model: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5b89b333554e31af19379d571128d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0            7            0            0            1            0\n",
      "Actual 1            0            9            1            0            0\n",
      "Actual 2            0            0           11            0            0\n",
      "Actual 3            0            0            0           13            0\n",
      "Actual 4            0            0            0            0            8\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.96\n",
      "               Precision  Recall  F1 Score  No of Actual Samples\n",
      "0               1.000000   0.875  0.933333                   8.0\n",
      "1               1.000000   0.900  0.947368                  10.0\n",
      "2               0.916667   1.000  0.956522                  11.0\n",
      "3               0.928571   1.000  0.962963                  13.0\n",
      "4               1.000000   1.000  1.000000                   8.0\n",
      "Macro Average   0.969048   0.955  0.960037                  50.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting Using Distilbert-Base-Uncased Model on Validation Set: \")\n",
    "\n",
    "model_checkpoint = bestModelCheckPoints[2]\n",
    "# Load the best model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenizedValidationDataset = validationDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define the Trainer with the best model\n",
    "trainer = Trainer(\n",
    "    model = model\n",
    ")\n",
    "\n",
    "predictions, label_ids, metrics = trainer.predict(tokenizedValidationDataset)\n",
    "predictedLabels_ForDistilbertBaseUncased = np.argmax(predictions, axis = 1)\n",
    "DistilBertBase_Precision, DistilBertBase_Recall, DistilBertBase_F1Score = calculateMetrics(tokenizedValidationDataset['label'], predictedLabels_ForDistilbertBaseUncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f035cbe-15a5-4ccd-afd3-7af21244df8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Using Microsoft/BiomedNLP-BiomedBERT-Base-Uncased-Abstract Model: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85953880bd24873ba5f1ee544f041cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0            7            0            0            1            0\n",
      "Actual 1            0           10            0            0            0\n",
      "Actual 2            1            0            9            1            0\n",
      "Actual 3            0            0            1           12            0\n",
      "Actual 4            1            0            0            2            5\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.86\n",
      "               Precision    Recall  F1 Score  No of Actual Samples\n",
      "0               0.777778  0.875000  0.823529                   8.0\n",
      "1               1.000000  1.000000  1.000000                  10.0\n",
      "2               0.900000  0.818182  0.857143                  11.0\n",
      "3               0.750000  0.923077  0.827586                  13.0\n",
      "4               1.000000  0.625000  0.769231                   8.0\n",
      "Macro Average   0.885556  0.848252  0.855498                  50.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting Using Microsoft/BiomedNLP-BiomedBERT-Base-Uncased-Abstract Model on Validation Set: \")\n",
    "\n",
    "model_checkpoint = bestModelCheckPoints[3]\n",
    "# Load the best model\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract')\n",
    "tokenizedValidationDataset = validationDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define the Trainer with the best model\n",
    "trainer = Trainer(\n",
    "    model = model\n",
    ")\n",
    "\n",
    "predictions, label_ids, metrics = trainer.predict(tokenizedValidationDataset)\n",
    "predictedLabels_ForMicrosoftBiomedBERT = np.argmax(predictions, axis = 1)\n",
    "BiomedBert_Precision, BiomedBert_Recall, BiomedBert_F1Score = calculateMetrics(tokenizedValidationDataset['label'], predictedLabels_ForMicrosoftBiomedBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36318050-b98e-4b2c-a78c-900c3bfc59fc",
   "metadata": {},
   "source": [
    "##  Final Evaluation and Deployment\n",
    "*<font color='Purple'>Marks - 6</font>*\n",
    "\n",
    "**Task** - \n",
    "\n",
    "Load the best model (based on macro-F1 on the validation set) that was saved in the previous question using a text-classification pipeline.\n",
    "- Evaluate the best of the four fine-tuned models on the testing set.\n",
    "- State which model you used and report the per-class precision, recall and F1 score as well as the accuracy, macro precision, macro recall and macro F1 score.\n",
    "- Comment on the performance and discuss whether the quality is high enough to be deployed for the client. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f676f8aa-017b-47b1-bb70-d2bfe7d7b03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Using Bert-Base-Uncased Model: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b41911dea364630ac18a8fcaedee9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0           12            0            0            0            1\n",
      "Actual 1            0            7            0            0            0\n",
      "Actual 2            0            0            9            2            0\n",
      "Actual 3            0            0            0            6            0\n",
      "Actual 4            0            1            0            2           10\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.88\n",
      "               Precision    Recall  F1 Score  No of Actual Samples\n",
      "0               1.000000  0.923077  0.960000                  13.0\n",
      "1               0.875000  1.000000  0.933333                   7.0\n",
      "2               1.000000  0.818182  0.900000                  11.0\n",
      "3               0.600000  1.000000  0.750000                   6.0\n",
      "4               0.909091  0.769231  0.833333                  13.0\n",
      "Macro Average   0.876818  0.902098  0.875333                  50.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8768181818181817, 0.9020979020979022, 0.8753333333333334)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing Using Bert-Base-Uncased Model: \")\n",
    "\n",
    "model_checkpoint = bestModelCheckPoints[0]\n",
    "# Load the best model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizedTestDataset = testDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define the Trainer with the best model\n",
    "trainer = Trainer(\n",
    "    model = model\n",
    ")\n",
    "\n",
    "predictions, label_ids, metrics = trainer.predict(tokenizedTestDataset)\n",
    "predictedLabels_ForBertBaseUncased = np.argmax(predictions, axis = 1)\n",
    "calculateMetrics(tokenizedTestDataset['label'], predictedLabels_ForBertBaseUncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0589e6a8-140b-420c-a945-cd56c3aedc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Using Roberta-Base-Uncased Model: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2b86782c3d46d1bace9eef3784409e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0           13            0            0            0            0\n",
      "Actual 1            0            7            0            0            0\n",
      "Actual 2            0            0           11            0            0\n",
      "Actual 3            0            0            0            6            0\n",
      "Actual 4            4            1            1            1            6\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.86\n",
      "               Precision    Recall  F1 Score  No of Actual Samples\n",
      "0               0.764706  1.000000  0.866667                  13.0\n",
      "1               0.875000  1.000000  0.933333                   7.0\n",
      "2               0.916667  1.000000  0.956522                  11.0\n",
      "3               0.857143  1.000000  0.923077                   6.0\n",
      "4               1.000000  0.461538  0.631579                  13.0\n",
      "Macro Average   0.882703  0.892308  0.862236                  50.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.882703081232493, 0.8923076923076924, 0.8622355219151558)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing Using Roberta-Base-Uncased Model: \")\n",
    "\n",
    "model_checkpoint = bestModelCheckPoints[1]\n",
    "# Load the best model\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "tokenizedTestDataset = testDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define the Trainer with the best model\n",
    "trainer = Trainer(\n",
    "    model = model\n",
    ")\n",
    "\n",
    "predictions, label_ids, metrics = trainer.predict(tokenizedTestDataset)\n",
    "predictedLabels_ForRobertaBaseUncased = np.argmax(predictions, axis = 1)\n",
    "calculateMetrics(tokenizedTestDataset['label'], predictedLabels_ForRobertaBaseUncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9f35c9c7-34b2-409d-913e-e9df50c6612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Using Distilbert-Base-Uncased Model: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc29596c54794074ac47e9d39d76acb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0           12            0            0            0            1\n",
      "Actual 1            0            7            0            0            0\n",
      "Actual 2            0            0           10            0            1\n",
      "Actual 3            0            0            0            6            0\n",
      "Actual 4            0            1            2            2            8\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.86\n",
      "               Precision    Recall  F1 Score  No of Actual Samples\n",
      "0               1.000000  0.923077  0.960000                  13.0\n",
      "1               0.875000  1.000000  0.933333                   7.0\n",
      "2               0.833333  0.909091  0.869565                  11.0\n",
      "3               0.750000  1.000000  0.857143                   6.0\n",
      "4               0.800000  0.615385  0.695652                  13.0\n",
      "Macro Average   0.851667  0.889510  0.863139                  50.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8516666666666668, 0.8895104895104895, 0.8631387163561076)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing Using Distilbert-Base-Uncased Model: \")\n",
    "\n",
    "model_checkpoint = bestModelCheckPoints[2]\n",
    "# Load the best model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenizedTestDataset = testDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define the Trainer with the best model\n",
    "trainer = Trainer(\n",
    "    model = model\n",
    ")\n",
    "\n",
    "predictions, label_ids, metrics = trainer.predict(tokenizedTestDataset)\n",
    "predictedLabels_ForDistilbertBaseUncased = np.argmax(predictions, axis = 1)\n",
    "calculateMetrics(tokenizedTestDataset['label'], predictedLabels_ForDistilbertBaseUncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87a49e94-f19c-4dc4-8688-796513072dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Using Biomed-Base-Uncased Model: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a9d8d7ba8844f0a0d57f34f41af79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0           11            0            0            2            0\n",
      "Actual 1            0            6            0            1            0\n",
      "Actual 2            0            0            9            1            1\n",
      "Actual 3            0            0            0            5            1\n",
      "Actual 4            0            1            1            1           10\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.82\n",
      "               Precision    Recall  F1 Score  No of Actual Samples\n",
      "0               1.000000  0.846154  0.916667                  13.0\n",
      "1               0.857143  0.857143  0.857143                   7.0\n",
      "2               0.900000  0.818182  0.857143                  11.0\n",
      "3               0.500000  0.833333  0.625000                   6.0\n",
      "4               0.833333  0.769231  0.800000                  13.0\n",
      "Macro Average   0.818095  0.824809  0.811190                  50.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.818095238095238, 0.8248085248085248, 0.8111904761904762)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing Using Biomed-Base-Uncased Model: \")\n",
    "\n",
    "model_checkpoint = bestModelCheckPoints[3]\n",
    "# Load the best model\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract')\n",
    "tokenizedTestDataset = testDataset.map(lambda x: tokenize_function(x, tokenizer), batched = True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define the Trainer with the best model\n",
    "trainer = Trainer(\n",
    "    model = model\n",
    ")\n",
    "\n",
    "predictions, label_ids, metrics = trainer.predict(tokenizedTestDataset)\n",
    "predictedLabels_ForBiomedBaseUncased = np.argmax(predictions, axis = 1)\n",
    "calculateMetrics(tokenizedTestDataset['label'], predictedLabels_ForBiomedBaseUncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "61850f5a-3e95-43ab-bebe-ef8df18d5a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [{'label': 'LABEL_4', 'score': 0.41913554072380066}, {'label': 'LABEL_2', 'score': 0.8944077491760254}, {'label': 'LABEL_1', 'score': 0.9886124730110168}, {'label': 'LABEL_0', 'score': 0.7992284893989563}, {'label': 'LABEL_1', 'score': 0.986764132976532}, {'label': 'LABEL_3', 'score': 0.9357056021690369}, {'label': 'LABEL_3', 'score': 0.9144816398620605}, {'label': 'LABEL_4', 'score': 0.7727954387664795}, {'label': 'LABEL_4', 'score': 0.9780265688896179}, {'label': 'LABEL_2', 'score': 0.9484862685203552}, {'label': 'LABEL_3', 'score': 0.9770360589027405}, {'label': 'LABEL_4', 'score': 0.9284961223602295}, {'label': 'LABEL_1', 'score': 0.9858091473579407}, {'label': 'LABEL_4', 'score': 0.9755443334579468}, {'label': 'LABEL_0', 'score': 0.9643268585205078}, {'label': 'LABEL_2', 'score': 0.9526277780532837}, {'label': 'LABEL_0', 'score': 0.9632870554924011}, {'label': 'LABEL_2', 'score': 0.9493947625160217}, {'label': 'LABEL_4', 'score': 0.9180883169174194}, {'label': 'LABEL_0', 'score': 0.9612796902656555}, {'label': 'LABEL_1', 'score': 0.9875147342681885}, {'label': 'LABEL_4', 'score': 0.7318797707557678}, {'label': 'LABEL_1', 'score': 0.9685580730438232}, {'label': 'LABEL_2', 'score': 0.8299828171730042}, {'label': 'LABEL_3', 'score': 0.5787132382392883}, {'label': 'LABEL_4', 'score': 0.9747384786605835}, {'label': 'LABEL_2', 'score': 0.9495301246643066}, {'label': 'LABEL_0', 'score': 0.9623415470123291}, {'label': 'LABEL_2', 'score': 0.9508902430534363}, {'label': 'LABEL_3', 'score': 0.6980839371681213}, {'label': 'LABEL_4', 'score': 0.9754796624183655}, {'label': 'LABEL_1', 'score': 0.9880732893943787}, {'label': 'LABEL_1', 'score': 0.9886680245399475}, {'label': 'LABEL_0', 'score': 0.9360871911048889}, {'label': 'LABEL_0', 'score': 0.9063180685043335}, {'label': 'LABEL_3', 'score': 0.9794817566871643}, {'label': 'LABEL_3', 'score': 0.8759774565696716}, {'label': 'LABEL_3', 'score': 0.9735696315765381}, {'label': 'LABEL_2', 'score': 0.9667670726776123}, {'label': 'LABEL_3', 'score': 0.9592211842536926}, {'label': 'LABEL_0', 'score': 0.967390775680542}, {'label': 'LABEL_0', 'score': 0.9638397693634033}, {'label': 'LABEL_3', 'score': 0.963468074798584}, {'label': 'LABEL_4', 'score': 0.7340322136878967}, {'label': 'LABEL_0', 'score': 0.9493567943572998}, {'label': 'LABEL_0', 'score': 0.9641525149345398}, {'label': 'LABEL_1', 'score': 0.988218367099762}, {'label': 'LABEL_0', 'score': 0.9696858525276184}, {'label': 'LABEL_2', 'score': 0.9489150047302246}, {'label': 'LABEL_4', 'score': 0.9313437938690186}]\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "best_model_checkpoint = bestModelCheckPoints[0]\n",
    "\n",
    "# Create a text-classification pipeline\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model = best_model_checkpoint,\n",
    "    truncation = True\n",
    ")\n",
    "\n",
    "# Run inference on test set\n",
    "prediction = classifier([sample['content'] for sample in testSet])\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "37c34c25-a0d4-4aa0-a4d8-da7335da4d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4\n",
      "Actual 0           12            0            0            0            1\n",
      "Actual 1            0            7            0            0            0\n",
      "Actual 2            0            0            9            2            0\n",
      "Actual 3            0            0            0            6            0\n",
      "Actual 4            0            1            0            2           10\n",
      "\n",
      "\n",
      "\n",
      "Accuracy: 0.88\n",
      "               Precision    Recall  F1 Score  No of Actual Samples\n",
      "0               1.000000  0.923077  0.960000                  13.0\n",
      "1               0.875000  1.000000  0.933333                   7.0\n",
      "2               1.000000  0.818182  0.900000                  11.0\n",
      "3               0.600000  1.000000  0.750000                   6.0\n",
      "4               0.909091  0.769231  0.833333                  13.0\n",
      "Macro Average   0.876818  0.902098  0.875333                  50.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8768181818181817, 0.9020979020979022, 0.8753333333333334)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedTestLabels = [int(sample['label'][-1]) for sample in prediction]\n",
    "calculateMetrics([sample['label'] for sample in testSet], predictedTestLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a324435-6ef3-4c1f-91b3-919f9d12b5d7",
   "metadata": {},
   "source": [
    "## Generative AI usage\n",
    "*<font color='Purple'>Marks - 1</font>*\n",
    "\n",
    "**Task**\n",
    "- Report on whether and how you use generative AI in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf8a791-139f-4421-98e0-45a8d047c0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e03748a-e3ae-479d-9712-320650540225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
